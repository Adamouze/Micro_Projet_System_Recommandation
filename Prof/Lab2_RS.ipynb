{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Recommendation de film basée sur le contenu  (Content-Based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"ratings.csv\")\n",
    "movies = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le but est de construre  un moteur de recommandation basé sur le contenu qui calcule la similarité entre les films en fonction de leur genre. Il suggérera les films les plus similaires à un film particulier en fonction de son genre. Pour ce faire, on utilise le fichier movies.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposer la chaîne de caractère \"genre\" en un tableau de chaînes de caractères\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "# Convertire genres en string value\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction TfidfVectorizer de scikit-learn transforme le texte en vecteurs de caractéristiques pouvant être utilisés comme entrée de l'estimateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'min_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m tf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m,ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfit_transform(movies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m tfidf_matrix\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'min_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 0 instead."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(movies['genres'])\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On utilise ensuite **[Cosine Similarity] pour calculer pour calculer une quantité numérique qui indique la similarité entre deux films. Puisque nous avons utilisé le vecteur TF-IDF, le calcul du produit de points nous donnera directement le score de similarité cosinus. Par conséquent, nous utiliserons le **noyau_linéaire** de sklearn au lieu de cosinus_similarities car il est beaucoup plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarque: Si vous rencontrez des problèmes de mémoire ou d'exécution prolongée, vous pouvez calculer uniquement les similarités pour un sous-ensemble d'éléments ou pour les k-plus proches voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "\n",
    "# Exemple : calculer uniquement les similarités pour un échantillon\n",
    "subset_indices = np.random.choice(tfidf_matrix.shape[0], size=100, replace=False)\n",
    "subset_tfidf = tfidf_matrix[subset_indices]\n",
    "cosine_sim_subset = linear_kernel(subset_tfidf, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On dispose à présent d'une matrice de similarité cosinus par paire pour tous les films du jeu de données. L'étape suivante consiste à écrire une fonction qui renvoie les 20 films les plus similaires sur la base du score de similarité cosinus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])\n",
    "\n",
    "\n",
    "def genre_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:21]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essayons d'obtenir les meilleures recommandations pour quelques films et voyons si elles sont bonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_recommendations('Good Will Hunting (1997)').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_recommendations('Toy Story (1995)').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_recommendations('Saving Private Ryan (1998)').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une liste de recommandations pas trop mauvaise pour **Good Will Hunting** (Drame), **Toy Story** (Animation, Enfants, Comédie), et **Saving Private Ryan** (Action, Thriller, Guerre).\n",
    "\n",
    "Dans l'ensemble, voici les avantages de la recommandation basée sur le contenu :\n",
    "* Pas besoin de données sur les autres utilisateurs, donc pas de problèmes de démarrage à froid ou de rareté.\n",
    "* Peut recommander aux utilisateurs ayant des goûts uniques.\n",
    "* Possibilité de recommander des articles nouveaux et impopulaires.\n",
    "* Possibilité de fournir des explications sur les éléments recommandés en énumérant les caractéristiques du contenu à l'origine de la recommandation (dans le cas présent, les genres de films).\n",
    "\n",
    "Cette approche présente toutefois certains inconvénients :\n",
    "* Il est difficile de trouver les caractéristiques appropriées.\n",
    "* Elle ne permet pas de recommander des éléments en dehors du profil de contenu de l'utilisateur.\n",
    "* Impossibilité d'exploiter les jugements de qualité des autres utilisateurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Recommendation Model\n",
    "Dans ce qui suit et en complément du TP1, nous allons utiliser ici le filtrage collaboratif basé sur la mémoire pour faire des recommandations aux utilisateurs de films.\n",
    "L'idée est de construire une matrice de similarité.  la **matrice de similarité utilisateur** se compose de certaines mesures de distance qui mesurent la similarité entre deux paires d'utilisateurs. De même, la **matrice de similarité des items** mesure la similarité entre deux paires d'items.\n",
    "\n",
    "Trois mesures de similarité de distance sont généralement utilisées dans le cadre du filtrage collaboratif :\n",
    "1. **Similarité Jaccard** :\n",
    "    * La similarité est basée sur le nombre d'utilisateurs qui ont évalué les éléments A et B divisé par le nombre d'utilisateurs qui ont évalué soit A soit B\n",
    "    * Elle est typiquement utilisée lorsque nous n'avons pas d'évaluation numérique mais juste une valeur booléenne, comme l'achat d'un produit ou le clic sur une annonce.\n",
    "\n",
    "2. **la similarité cosinus** : (comme dans le système basé sur le contenu)\n",
    "    * La similarité est le cosinus de l'angle entre les 2 vecteurs des vecteurs des articles A et B.\n",
    "    * Plus les vecteurs sont proches, plus l'angle est petit et plus le cosinus est grand.\n",
    "\n",
    "3. **Similarité de Pearson** :\n",
    "    * La similarité est le coefficient de Pearson entre les deux vecteurs.\n",
    "\n",
    "Nous allons tester la **similarité de Pearson** dans cette mise en œuvre.\n",
    "\n",
    "### Mise en œuvre\n",
    "On utilise d'abord le fichier **ratings.csv** car il contient l'identifiant de l'utilisateur, l'identifiant du film et les évaluations. Ces trois éléments sont tout ce dont on a besoin pour déterminer la similarité des utilisateurs sur la base de leurs évaluations d'un film particulier.\n",
    "\n",
    "On commence par un  traitement rapide des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplir les valeurs NaN avec des 0\n",
    "ratings['userId'] = ratings['userId'].fillna(0)\n",
    "ratings['movieId'] = ratings['movieId'].fillna(0)\n",
    "\n",
    "# Remplacer les valeurs NaN dans la colonne de notation par la moyenne de toutes les valeurs\n",
    "ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En raison de la puissance de calcul limitée, nous n'utiliiserons qu'un échantillon aléatoire de 20 000 évaluations (2 %) sur les 1 million d'évaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_data = ratings.sample(frac=0.02)\n",
    "\n",
    "print(small_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise  la bibliothèque **scikit-learn** pour diviser l'ensemble de données en test et train.  La bibliothèque **Cross_validation.train_test_split** mélange et divise les données en deux ensembles de données en fonction du pourcentage d'exemples de test, qui dans ce cas est de 0,2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(small_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons créer une matrice utilisateur-item. Comme nous avons divisé les données en test et train, on doit créer deux matrices. La matrice de train contient 80 % des évaluations (ratings) et la matrice de test contient 20 % des évaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer deux matrices utilisateur-item, l'une pour le training et l'autre pour le test.\n",
    "\n",
    "# Convertir un DataFrame en tableau NumPy\n",
    "train_data_matrix = train_data[['userId', 'movieId', 'rating']].to_numpy()\n",
    "test_data_matrix = test_data[['userId', 'movieId', 'rating']].to_numpy()\n",
    "\n",
    "\n",
    "print(train_data_matrix.shape)\n",
    "print(test_data_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction **pairwise_distances** de sklearn permet de  calculer le [coefficient de corrélation de Pearson]. Cette méthode offre un moyen sûr de prendre une matrice de distance en entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Matrice de similarité des users\n",
    "user_correlation = 1 - pairwise_distances(train_data, metric='correlation')\n",
    "user_correlation[np.isnan(user_correlation)] = 0\n",
    "print(user_correlation[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de similarité des items\n",
    "item_correlation = 1 - pairwise_distances(train_data_matrix.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0\n",
    "print(item_correlation[:4, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la matrice de similarité on peut maintenant prédire les évaluations qui n'ont pas été incluses dans les données. À l'aide de ces prédictions, on peut ensuite les comparer aux données de test pour tenter de valider la qualité de notre modèle de recommandation.\n",
    "\n",
    "Dans le cas du user-user CF, on considère la similarité entre deux utilisateurs (A et B, par exemple) comme des poids qui sont multipliés par les évaluations d'un utilisateur B similaire (corrigées en fonction de l'évaluation moyenne de cet utilisateur). On doit également normaliser le tout pour que les notes restent comprises entre 1 et 5 et, dans une dernière étape, additionner les notes moyennes de l'utilisateur que nous essayons de prédire. L'idée est que certains utilisateurs peuvent avoir tendance à donner des notes élevées ou faibles à tous les films. La différence relative entre les notes attribuées par ces utilisateurs est plus importante que les valeurs absolues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour prédir des notes\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        # Use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Il existe de nombreuses mesures d'évaluation, mais l'une des mesures les plus populaires utilisées pour évaluer la précision des évaluations prédites est l'**erreur quadratique moyenne de la racine (RMSE)**. Nous allons utiliser la fonction **mean_square_error (MSE)** de sklearn, où la RMSE est simplement la racine carrée de la MSE.\n",
    "\n",
    "$$\\mathit{RMSE} =\\sqrt{\\frac{1}{N} \\sum (x_i -\\hat{x_i})^2}$$.\n",
    "\n",
    "On utilisera la fonction **moyenne des erreurs au carré** de scikit-learn comme mesure de validation. En comparant le filtrage collaboratif basé sur l'utilisateur et le filtrage collaboratif basé sur l'item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Fonction de calcul de RMSE\n",
    "def rmse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les évaluations sur les données d'apprentissage avec les deux scores de similarité\n",
    "user_prediction = predict(train_data_matrix, user_correlation, type='user')\n",
    "item_prediction = predict(train_data_matrix, item_correlation, type='item')\n",
    "\n",
    "# RMSE sur les données de test\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE sur le train\n",
    "print('User-based CF RMSE: ' + str(rmse(user_prediction, train_data_matrix)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(item_prediction, train_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La RMSE dest une mesure qui permet d'évaluer dans quelle mesure le signal et le bruit sont expliqués par le modèle. Nous obtenons un RMSE  assez élevé lié à un sur-apprentissage.\n",
    "\n",
    "Dans l'ensemble, le filtrage collaboratif basé sur la mémoire est facile à mettre en œuvre et produit une qualité de prédiction raisonnable. Cependant, cette approche présente certains inconvénients :\n",
    "\n",
    "* Elle n'aborde pas le problème bien connu du démarrage à froid, c'est-à-dire lorsqu'un nouvel utilisateur ou un nouvel élément entre dans le système. \n",
    "* Elle ne peut pas traiter les données éparses, ce qui signifie qu'il est difficile de trouver des utilisateurs qui ont évalué les mêmes articles.\n",
    "* Il souffre de l'arrivée de nouveaux utilisateurs ou d'éléments qui n'ont pas été évalués.\n",
    "* Il a tendance à recommander des articles populaires."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
